---
title: "No free lunch when estimating simulation parameters"
date: "`r Sys.Date()`"
abstract: "We estimate the parameters of 21 simulation models using 9 estimation algorithms to discover which one is better at matching simulations to data. Unfortunately no single algorithm is best at minimizing estimation error for all or even most the simulations; the best algorithm differs for each simulation, and sometimes for each parameter of each simulation. Cross-validation is necessary to pair each simulation's parameter to its appropriate estimation algorithm. This is computationally feasible for reference table algorithms. In terms of confidence intervals the results are more clear: bootstrap generates more precise prediction intervals than either quantiles or Approximate Bayesian Computation."
author:
- name: Ernesto Carrella^[Corresponding author:ernesto.carrella@ouce.ox.ac.uk]
  affiliation: School of Geography and the Environment, University of Oxford, South Parks Road, Oxford, OX1 3QY, UK.
header-includes:
 - \usepackage{algorithmic}
 - \usepackage{algorithm}
output: 
  bookdown::pdf_document2:
    keep_tex: true
  bookdown::word_document2:
#    number_sections: true
    fig_caption: true
  bookdown::html_document2:
      toc: true
      toc_depth: 3
      theme: "readable"
      highlight: haddock
      number_sections: true
      self_contained: true
bibliography: ["library.bib"]
biblio-style: "apalike"
link-citations: true
urlcolor: blue
editor_options: 
  chunk_output_type: console
---  

<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-58894263-1', 'auto');
  ga('send', 'pageview');

</script>


# Keywords {-}

Agent-based models;
Individual-based models;
Estimation;
Calibration;
Approximate Bayesian Computation;
Random Forest;
Generalized Additive Model;
Bootstrap;



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning= FALSE, message=FALSE,
                      cache=TRUE,
                      dpi = 300, fig.width = 13.3, fig.height = 10)
library(lme4) #lmer fit at the end
library(tidyverse)
library(here)


path<-"/home/carrknight/code/oxfish/docs/indirect_inference/commondata/"

methods_used<- c("Rejection ABC"="rejection",
                 "Semi-automatic ABC 4D"="sabc_4d",
                 "Semi-automatic ABC 1D"="sabc_1d",
                 "Local-linear ABC" = "loclin",
                 "Neural Network ABC" = "neuralnet",
                 "Linear Regression" = "lm",
                 "GAM"="loess",
                 "Quantile Random Forest"="rf",
                 "Regression Random Forest"="rfboot",
                 "Average"="average")

experiments_list<-
  c("alfastronk_medium",
"alfastronk_small",
"birds_simple",
"birds",
"brokenliner_medium",
"brokenliner_small",
"coal_quick",
"earthwormjim",
"gk_medium",
"gk_small",
"hierarchy_medium",
"hierarchy_small",
"lk_noisy",
"lk_nonoise_quick",
"local_medium",
"local_small",
"ma_medium",
"ma_small",
"medmad_medium",
"medmad_small",
"medmadgarbage_medium",
"medmadgarbage_small",
"musigma2",
"normal25_medium",
"normal25_small",
"overfat_medium",
"overfat_small",
"overfit_medium",
"overfit_small",
"partial_medium",
"partial_small",
"rbc_full",
"rbc",
"steel_quick",
"toymodel_medium",
"toymodel_small",
"traits_medium",
"traits_small",
"wilkinson_medium",
"wilkinson_small")

experiment_label<-
   c("alpha-stable 5,000",
"alpha-stable 1,250",
"Birds ABM - 2 SS",
"Birds ABM - 105 SS",
"Broken Line 5,000",
"Broken Line 1,250",
"Coalescence",
"Earthworm",
"g-k distribution 5,000",
"g-k distribution 1,250",
"Hierarchical Normal Mean 1,250",
"Hierarchical Normal Mean 5,000",
"Lotke-Volterra Noisy",
"Lotke-Volterra Non-Noisy",
"Locally Identifiable 5,000",
"Locally Identifiable 1,250",
"Moving Average 5,000",
"Moving Average 1,250",
"Median and MAD 5,000 - 2 SS",
"Median and MAD 1,250 - 2 SS",
"Median and MAD 5,000 - 4 SS",
"Median and MAD 1,250 - 5 SS",
"mu-sigma",
"Normal 25 5,000",
"Normal 25 1,250",
"Scale 5,000",
"Scale 1,250",
"Unidentifiable 5,000",
"Unidentifiable 1,250",
"Partially Identifiable 5,000",
"Partially Identifiable 1,250",
"Real Business Cycle - 48 SS",
"Real Business Cycle - 44 SS",
"Pathogen",
"Toy Model 1,250",
"Toy Model 5,000",
"Ecological Traits 5,000",
"Ecological Traits 1,250",
"Wilkinson 5,000",
"Wilkinson 1,250")


```

# Introduction

Models take parameters as input to describe and predict a phenomenon.
When we gather real data we can search for the input parameters that most likely generated it.
This is difficult for simulation models whose likelihood function is often unknown or intractable.
Fortunately, many likelihood-free estimation algorithms are available[@hartig_statistical_2011]. 
We would like to know if any of them estimate parameters better, particularly when data is high dimensional, simulating is computationally expensive and in the presence of identification issues.

In this paper we estimate the parameters of 21 simulations with nine different algorithms. 
We focus on "reference table" algorithms: rejection ABC [Approximate Bayesian Computation as in @Beaumont2002], regression-adjusted ABC and regression-only methods.
We rank them by their estimation error and the quality of their confidence intervals.
Superficially, there is a winner: GAM, generalized additive models, regressions [@Hastie1986; @Wood2002] produce most often the lowest estimation errors, the best confidence intervals and no identification failures.
Even so, GAMs produce the best estimates for only 30% of the parameters. In fact, each of the algorithms tested is the best estimator in at least one instance.

Since best estimation algorithm changes between simulations and even between parameters of the same simulation, there is no alternative to test multiple algorithms against each parameter of a simulation.
Fortunately this plays to the advantage of reference table algorithms since the data used to train one algorithm can be recycled to train all the others as well as rank them by cross-validation.




    
# Materials and Methods

We define here a simulation model as any function that depends on a set of parameter $\theta$ to generate a set of summary statistic $S(\theta)$.
We are interested in the estimation problem where we observe summary statistics $S^*$ and we want to know which parameter $\theta^*$ most likely generated them.

We parametrize 21 simulation models (described in section \@ref(experiments)).
We have nine candidate algorithms to do so (described in section \@ref(algorithms)).
All are "reference table" algorithms: algorithms whose only input for estimation is a table of simulation parameters $\theta$, selected by random sampling, and the summary statistics $S(\theta)$ they generate.
We split this reference table into training and testing sets and ask each algorithm to estimate the parameters of the testing set observing only the training portion of the reference table.


We measure algorithm performance with two indicators: predictivity  and coverage.
Predictivity [@Salle2014; modelling efficiency in @Stow2009] is the out of sample mean square error when using a particular algorithm normalized by the mean square error of using the average parameter instead:
\begin{equation} 
 1 - \frac{ \sum \left( \theta_i - \hat \theta_i \right)^2}{ \sum \left( \theta_i - \bar \theta \right)^2} 
(\#eq:predictivitycoefficient)
\end{equation}
Where $\hat \theta$ is the estimated parameter, $\theta$ is the real simulation parameter and $\bar \theta$ is the average parameter value. 
Predictivity ranges from 1 (perfectly estimated) to 0 (unidentified) to negative values (misidentified).

We define coverage as in @MarixnArxiv as the percentage of times the real parameter falls within the 95% prediction intervals suggested by the estimating algorithm. The best coverage is 95%: higher generates type I errors, lower generates type II errors.




## Models {#experiments}



We estimate the parameters of 21 separate simulations, repeating some with different amount of data or summary statistics.
We can roughly categorize the simulations into three groups: simple, ill posed and complicated.
Table \@ref(tab:experimentsmasterlist) lists them all.


Table: (\#tab:experimentsmasterlist)  List of all the models  parametrized

| Experiment               | No. of parameters | No. of summary statistics | No. of simulations | Testing            |
|--------------------------|-------------------|---------------------------|--------------------|--------------------|
| $\alpha$-stable          | 3                 | 11                        | 1,250 or 5,000     | 5-fold CV          |
| Birds ABM                | 2                 | 2 or 105                  | 5,000              | 5-fold CV          |
| Broken Line              | 1                 | 10                        | 1,250 or 5,000     | 5-fold CV          |
| Coalescence              | 2                 | 7                         | 100,000            | Single testing set |
| Earthworm                | 11                | 160                       | 100,000            | Single testing set |
| $g$-and-$k$ distribution | 4                 | 11                        | 1,250 or 5,000     | 5-fold CV          |
| Hierarchical Normal Mean | 2                 | 61                        | 1,250 or 5,000     | 5-fold CV          |
| Lotke-Volterra           | 2                 | 16 (noisy or non-noisy)   | 100,000            | Single testing set |
| Locally Identifiable     | 2                 | 2                         | 1,250 or 5,000     | 5-fold CV          |
| Moving Average (2)       | 2                 | 2                    | 1,250 or 5,000     | 5-fold CV          |
| Median and MAD           | 2                 | 2 or 4                    | 1,250 or 5,000     | 5-fold CV          |
| $\mu$-$\sigma^2$         | 2                 | 2                         | 10,000             | 5-fold CV          |
| Normal 25                | 2                 | 25                        | 1,250 or 5,000     | 5-fold CV          |
| Scale                    | 2                 | 1                         | 1,250 or 5,000     | 5-fold CV          |
| Unidentifiable           | 2                 | 1                         | 1,250 or 5,000     | 5-fold CV          |
| Partially Identifiable   | 2                 | 2                         | 1,250 or 5,000     | 5-fold CV          |
| Real Business Cycle      | 6                 | 44 or 48                  | 2,944 or 2,961     | 5-fold CV          |
| Pathogen                 | 4                 | 11                        | 200,000            | Single testing set |
| Toy Model                | 2                 | 2                         | 1,250 or 5,000     | 5-fold CV          |
| Ecological Traits        | 4                 | 4                         | 1,250 or 5,000     | 5-fold CV          |
| Wilkinson                | 1                 | 1                         | 1,250 or 5,000     | 5-fold CV          |


```{r}
#get all the files
error_path<-paste(path,"errors",sep="")
error_table<-list.files(error_path, ".csv")  %>% 
  #turn the vector into a data.frame (name-value)
  enframe() %>%
  #read each file
  mutate( observations = map(file.path(error_path,value),~read_csv(.))) %>%
  #clean up names
  select(-name) %>% rename(experiment=value) %>% 
  mutate(experiment=gsub(".csv","",experiment)) %>%
  #unnest results and you are done
  unnest(observations)  %>%
  #remove M earthworm which is always 0 anyway
  filter(!(experiment=="earthwormjim" & parameter=="M"))



  

coverage_path<-paste(path,"contained",sep="")
coverage_table<-list.files(coverage_path, ".csv")  %>% 
  #turn the vector into a data.frame (name-value)
  enframe() %>%
  #read each file
  mutate( observations = map(file.path(coverage_path,value),~read_csv(.))) %>%
  #clean up names
  select(-name) %>% rename(experiment=value) %>% 
  mutate(experiment=gsub(".csv","",experiment)) %>%
  #unnest results and you are done
  unnest(observations)  %>%
  #remove M earthworm which is always 0 anyway
  filter(!(experiment=="earthwormjim" & parameter=="M"))



## who's best in terms of predictivity?
error_table %>% filter(method=="average") %>% 
    rename(average=value) %>% select(-method) -> 
  averages



predictivity<-
  left_join(error_table,averages) %>%
 # complete(experiment,method,fill=list(value=NA)) %>%
  mutate(predictivity=1-value/average) 

```



### Simple simulations

Simple simulations, few parameters and summary statistics, feature prominently in the ABC literature both as a teaching tool and to compare different techniques.
They are useful because they run quickly but they may bias comparisons towards simpler estimation algorithms.
We compute predictivity and coverage for all the experiments in this section by 5-fold cross-validation: keeping one fifth of the data out of sample, using the remaining portion to train our algorithms and doing this five times, rotating each time the portion of data used for testing.
We run all the experiments in this section twice: once the total data is 1,250 simulation runs  and once it is 5,000 simulation runs.

*$\alpha$-stable*: @Rubio2013 uses ABC to recover the parameters of an $\alpha$-stable distribution by looking at sample of 1096 independent observations from it. We replicate this here using the original priors for the three parameters ($\alpha \sim U(1,2)$, $\mu \sim U(-0.1,0.1)$, $\sigma \sim U(0.0035,0.0125)$). We use 11 summary statistics representing the 0%,10%,$\dots$,100% deciles of each sample generated.

*$g$-and-$k$ distribution*: @Karabatsos2017 uses ABC to estimate the parameters of the g-and-k distribution (an extension of the normal distribution whose density function has no analytical expression). We replicate this here using the `gk` package in R [@Prangle2017].
We want to retrieve the 4 parameters of the distribution $A,B,g,k \sim U[0,10]$ given the 11 deciles (0%,10%,...,100%) of a sample of 1,000 observations from that distribution.

*Normal 25*: Sometimes sufficient summary statistics exist but the modeller may miss them and use others of lower quality.
In this example 25 i.i.d observations from the same normal distribution $\sim N(\mu,\sigma^2)| \mu \sim U(-5,5); \sigma \sim U(1,10)$ are used directly as summary statistics to retrieve the two distribution parameters $\mu,\sigma^2$.

*Moving Average(2)*: @Creel2017 used neural networks to recover the parameters of the MA(2) process with $\beta_1 \sim U(-2,2); \beta_2\sim U(-1,1)$. We observe generated a time series of size 100 and we summarise it with the coefficients a AR(10) regression.

*Median and MAD*:  As a simple experiment we sample 100 observations from a normal distribution $\mu \sim U(-5,5)$ and $\sigma \sim U(0.1,10)$ and we collect as summary statistics their median and median absolute deviation, using them to retrieve the original distributions. 
We run this experiment twice, the second time adding two useless summary statistics $S_3 \sim N(3,1)$ and $S_4 \sim N(100,.01)$.

*$\mu$-$\sigma^2$*: The `abc` package in R [@Csillery2012] provides a simple dataset example connecting two observed statistics: "mean"" and "variance" as" generated by the parameters $\mu$ and $\sigma^2$. The posterior that connects the two derives from the Iris setosa observation [@Anderson1935].
The data set contains 10,000 observations and we log-transform $\sigma^2$ when estimating.

*Toy Model*:  A simple toy model suggested by the `EasyABC` R package[@Jabot2013]  involves retrieving two parameters, $a \sim U[0,1]; b \sim U[1,2]$, observing two summary statistics $S_1 = a + b + \epsilon_1 ; S_2 = a b +\epsilon_2 | \epsilon_1,\epsilon_2 \sim N(0,.1^2)$.


*Ecological Traits*: The `EasyABC` R package[@Jabot2013] provides a replication of @Jabot2010, a trait-based ecological simulator. Here we fix the number of individuals to 500 and the number of traits to 1, leaving four free parameters: $I \sim U(3,5),A\sim U(0.1,5),h\sim U(-25,125),\sigma\sim U(0.5,25)$. We want to estimate these with four summary statistics: richness of community $S$, shannon index $H$, mean and skewness of traiv values in the community.


*Wilkinson*: @wilkinsonnips suggested a simple toy model with one parameter, $\theta \sim U(-10,10)$, and one summary statistic $S_1 \sim N(2 (\theta + 2) \theta(\theta-2), 0.1 + \theta^2)$.
We run this experiment twice, once where the total data is 1,250 sets of summary statistics and one where the total data is 5,000 sets of summary statistics.

### Ill-posed Models

We often face identification issues: the inability to recover parameters given the information we have.
Because these issues take many forms, we produce a series of experiments to test each.
As with simple simulations, we test all the experiments with 5-fold cross validation and run each twice: once where the total reference table has 1,250 total rows, and once where it has 5,000.

Ideally two behaviours should emerge from an estimation algorithm under these circumstances. 
First we would like to maximize the quality of our estimated parameters when the information is noisy (the lesser problem of "weak" identification).
Second we would like our estimation algorithm to recognize when the model cannot be identified and not be fooled into still producing an arbitrary estimate and a small confidence interval around it.

*Broken Line*:  we observe 10 summary statistics $S=(S_0,\dots,S_9)$ generated by: 
$$
S_i=\left\{\begin{matrix}
\epsilon & i < 5\\ 
\beta i + \epsilon & i\geq5
\end{matrix}\right. (\#eq:brokenline)  
$$ 
and where $\beta \sim U(0,2)$

*Hierarchical Normal Mean*: @MarixnArxiv compares ABC to direct random forest estimation in a "toy" hierarchical normal mean model:
$$
\left.\begin{matrix}
y_i |\theta_1,\theta_2 \sim N(\theta_1,\theta_2) \\ 
\theta_1|\theta_2 \sim N(0,\theta_2) \\ 
\theta_2 \sim IG(\kappa,\lambda)
\end{matrix}\right. (\#eq:hierarchicalnormalmean)  
$$ 
Where $IG(\cdot)$ is the inverse gamma distribution. We want to estimate $\theta_1,\theta_2$ given a sampled vecor $y$ of size 10 which is described by 61 summary statistics: the mean, the variance, the median absolute deviation of the sample, all possible combinations of their products and sums as well as 50 noise summary statistics $\sim U(0,1)$.

*Locally Identifiable*: macroeconomics often deals with structural models that are only locally identifiable [see @Fernandez-Villaverde2015]. These are models where the true parameter is only present in the data for some of its possible values. Here we use the example:
$$
S_i=\left\{\begin{matrix}
y \sim N(\theta_1,\theta_2) & \theta_1>2, \theta_2>2\\ 
y \sim N(0,1)  &  \text{Otherwise}
\end{matrix}\right. (\#eq:local)  
$$ 
Where $\theta_1,\theta_2 \sim U[0.1,5]$, each simulation we sample the vector $y$ of size 100 and we collect its mean and standard deviation as summary statistics.

*Scale*: a common source of under-identification in economics occurs when "when two structural parameters enter the objective function only proportionally, making them separately unrecoverable"[@Canova2005].
In this example, two people of weight $w_1,w_2\sim U[80,150]$ step together on a scale whose reading $S_1 = w_1 + w_2 + \epsilon | \epsilon \sim N(0,1)$ is the only summary statistic we can use.
This problem is locally identifiable to an extent: very low readings means both people are light (and viceversa).

*Unidentifiable*:  in some cases the model parameters are just unrecoverable and we hope that our estimation algorithm does not tell us otherwise.
In this example the three summary statistics $S_1,S_2,S_3 \sim N(x,1)| x \sim U[0,50]$ provide no information regarding the two parameters we are interested in: $\mu\sim U(0,50), \sigma \sim U(0,25)$.

*Partially Identifiable*: @Fernandez-Villaverde2015 mention how partial identification can occur when a model is the real data generating process conditional on some other unobserved parameter. This makes the model identifiable in some samples but not others. The example we use is a slight modification of the original: we try to retrieve parameter $\theta \sim U[1,5]$ when we observe mean and standard deviation of a size 10 vector $y$ generated as follows:
$$
y \sim N(\theta\cdot x,1), ~
x=\left\{\begin{matrix}
0 & \text{with probability } \frac 1 2\\ 
 \sim N(1,1)  &  \text{Otherwise}
\end{matrix}\right. (\#eq:partial)  
$$ 

### Complicated Models

Simulations, particularly agent-based models, tend to be large. They involve many input parameters and summary statistics. 
Inference in a high-dimensional space is difficult not just for ABC methods but for non-parametric smoothing as well [see section 4.5 in @Wasserman2006].
In principle one could solve this by just simulating more data but complicated simulation models tend also to be slow.
This puts a premium on the quality of the algorithms to extrapolate from the data they have.


*Birds ABM*: @Thiele2014 estimated the parameters of a simple agent-based bird population model (originally in @Railsback2011) with ABC. Their paper provided an open source NETLOGO implementation of the model. The model depends on two parameters: `scout-prob`$\sim U[0,0.5]$ and `survival-prob`$\sim U[0.95,1]$. We ran this experiment twice, once where there are only 2 summary statistics: mean abundance and mean variation over 20 years, and one where are 105 (comprising the average, last value, standard deviation, range and the coefficients of fitting an AR(5) regression to the time series of abundance, variation, months spent foraging and average age within bird population). 
This experiment is useful because in the original specification (with 2 summary statistics) the `scout-prob` parameter is unidentifiable.
For each experiment we ran the model 5000 times.


*Coalescence*: the `abctools` package [@Nunes2015] provides 100,000 observations of 7 summary statistics from a DNA coalescent model depending on two parameters $\theta \sim u[2,10]$ and $\rho \sim U[0,10]$. @Blum2013 in particular used this dataset to compare the quality of ABC dimensionality reduction schemes to better estimate the two parameters.
This data-set is too big for cross-validation so in this experiment we simply used 1,250 observation as the testing data-set and the rest for training.



*Lotke-Volterra*: @Toni2009 showcases SMC-ABC with a 2 species deterministic Lotke-Volterra model with 2 parameters: $a,b$.
$$
\left\{\begin{matrix}
 \frac{dx}{dt} = ax - yx \\ 
 \frac{dy}{dt} = bxy - y  
\end{matrix}\right.
$$ 
Here we assume $a,b \sim U(0,10)$ (avoiding the negative values in the original paper).  For each simulation we sample 8 observations for predator and prey at time $t=1,1.2, 2.4, 3.9, 5.7, 7.5, 9.6, 11.9, 14.5$ (as in the original paper). 
We run this experiment twice, once where data is observed perfectly and one where to each observation we add noise $\sim N(0,0.5)$.
In both experiments we do not perform 5-fold cross validation, rather we generate 100,000 sets of summary statistics for training and another 1,250 sets of summary statistics to test the parametrization.


*Real Business Cycle*: we want to parametrize the default Real Business Cycle model (a simple but outdated class of macro-economics models) implemented in the `gEcon` R package[@Klima2018].
It has 6 parameters ($\beta,\delta,\eta,\mu,\phi,\sigma$) and we try to parametrize them in two separate experiments.
In the first, we use as summary statistics the -10,+10 cross-correlation table between output $Y$, consumption $C$, investment $I$, interest rates $r$ and employment $L$ (44 summary statistics in total). For this experiment we have 2,944 distinct observations.
In the second experiment we follow @Carrella2018 using as summary statistics (i) coefficients of regressing $Y$ on $Y_{t-1},I_{t},I_{t-1}$, (ii) coefficients of regressing $Y$ on $Y_{t-1},C_{t},C_{t-1}$, (iii)  coefficients of regressing $Y$ on $Y_{t-1},r_{t},r_{t-1}$, (iv) coefficients of regressing $Y$ on $Y_{t-1},L_{t},L_{t-1}$, (v) coefficients of regressing $Y$ on $C,r$ (vi) coefficients of fitting AR(5) on $Y$, (vii)  the (lower triangular) covariance matrix of $Y,I,C,r,L$. 48 summary statistics in total. For this experiment we have 2,961 distinct observations.

*Pathogen*: another dataset used by  @Blum2013 to test dimensionality reduction methods for ABC concerns the ability to predict pathogens' fitness changes due to antibiotic resistance [the original model and data is from @Francis2009]. The model has four free parameters and 11 summary statistics.
While the original data-set contains 1,000,000 separate observations, we only sample 200,000 at random for training the algorithms and 1,250 more for testing.

*Earthworm*: @VanderVaart2015 calibrated an agent-based model of earthworms with rejection ABC. The simplified version of the model contains 11 parameters and 160 summary statistics. The original paper already carried out cross-validation proving under-identification: the model contains a mixture of unidentified, weakly identified and well identified parameters.
We use 100,000 runs from the original paper, setting 1,250 aside for out of sample testing and using the rest for training.




## Algorithms {#algorithms}

We test nine algorithms to parametrize simulations: five are ABC and four are regressions-only.
In this paper we focused exclusively on reference table algorithms. 
We thus ignored methods that combine search and estimation, such as synthetic likelihood [@Wood2010;@fasiolo2014introduction], ABC-MCMC [@hartig_statistical_2011] and Bayesian optimization [@Snoek2012].
We also ignored regression-only techniques that do not generate prediction intervals such as the deep neural networks proposed in @Creel2017 and the elastic nets proposed in @Carrella2018.



### ABC

The first algorithm we use is the simple rejection ABC [@Pritchard1999; @Beaumont2002]. Start by ranking all training observations by their euclidean distance to the testing summary statistics \( \left( \sum_i S_i(\theta) -  S_i(\theta^*) \right)^2 \). Ignore all training observations except the closest 10%. 
Use the $\theta$ parameters of the accepted observations to generate the posterior estimate for the testing parameter $\theta^*$.

The second algorithm is the local-linear regression adjusted ABC [@Beaumont2002].
Weigh all training observations by an Epanechnikov kernel with bandwidth equal to euclidean the distance between the testing summary statistics  $S(\theta^*)$ and the furthest $S(\theta)$ we would have accepted using simple rejection.
Then run a local-linear regression on the weighted training set to predict $E[\theta |S(\theta)]$ and use the residuals of that regression to estimate the posterior distribution for the testing parameter $\theta^*$.

The third algorithm, neural network ABC, feeds the same weighted training set to a feed forward neural network [@Blum2010].
The approach is similar to the local-linear regression above but the residuals are also weighted by a second regression (on the log squared residuals) to correct for heteroskedasticity.

These three algorithms are implemented in the `abc` package[@Csillery2012] in R. We used the package default settings for its neural networks (10 networks, 5 units in the hidden layer and weight decay randomly chosen for each network between $0.0001$,$0.001$ and $0.01$).

The fourth and fifth algorithm are semi-automatic ABC methods which "pre-process" summary statistics before applying rejection ABC[@Prangle2014].
More precisely, the original summary statistics $S(\theta)$ are fed into a set linear regressions estimating $r_i=E[\theta_i|S(\theta)]$ (one for each parameter of the model) and then regression fitted values are used as summary statistics for the simple rejection ABC.
The rationale is that these regressions will project the summary statistics into a space where rejection ABC performs better.
We do this in two different ways here: by running first or fourth degree linear regressions in the pre-processing phase.
This is done using the R package `abctools`[@Nunes2015] and their default parameters: using half of the training set to run the regression and the other half to run the rejection ABC.

A feature of all ABC methods is that they are local: they remove or weight training observations differently depending on the $\theta^*$ we want to estimate.
This means that during cross-validation we need to retrain each ABC for each row of the testing set.


### Regression Only

Estimating parameters by regression is a straightforward process.
We build a separate regression $r$ for each $\theta$ in the training set as dependent variable using the summary statistics $S(\theta)$ as the independent variables.
We then plug in these regressions the testing summary statistic $S(\theta^*)$ to predict the testing parameter $\theta^*$.
When a simulation depends on multiple parameters we build a separate regression for each parameter.

The simplest algorithm of this class is linear regression of degree one.
It is linear, its output is understandable and is fast to compute.
This speed allows us to estimate the prediction interval of $\theta^*$ by resampling bootstrap[@Davison]: we produce 200 training data sets by resampling with replacement from the original one and run the same linear regression on on each new data set. 
From each regression $i$ we collect their prediction $\beta_i S(\theta^*)$ and sample one standardized residual $e$ (a residual divided by the square root of one minus the hat value associated with that residual).
This produces a set of 200 $\beta_i S(\theta) + e_i$.
The 95% prediction interval is then defined by 2.5 and 97.5 percentile of this set.

A more complex algorithm that is not linear but still additive is the generalized additive model(GAM), where we regress:
\[ \theta = \sum s_i(S_i(\theta)) \]
$s_i$ is a smooth spline transformation [see chapter 9 in @friedman_elements_2001; also @Wood2002].
We use the `mgcv` R package[@Wood2017;@Wood2004].
The  boostrap prediction intervals techniques we used for linear regressions is too computationally expensive to replicate with GAMs.
Instead we produce prediction intervals by assuming normal standard errors (generated by the regression itself) and by resampling residuals directly: we generate 10,000 draws of $z(S(\theta))+\epsilon$ where $z$ is normally distributed with standard deviation equal to regression's standard error at $S(\theta)$ and $\epsilon$ is a randomly drawn residual of the original regression.  
The 95% prediction interval for $\theta^*$ is then defined by 2.5 and 97.5 percentile of the generated $z(S(\theta^*))+\epsilon$ set.

A completely non-parametric regression advocated in @MarixnArxiv is the random forest[@Breiman2001].
We implement this in two ways here. 
First, as a quantile random forest [@Meinshausen2006], using in `quantregForest` R package [@Meinshausen2017]; prediction intervals for any simulation parameter $\theta^*$ are the predicted 2.5 and 97.5 quantile at $S(\theta^*)$.
Second, as a regression random forest using the `ranger` and `caret` packages in R [@Wright2015;@Kuhn2008]. 
For this method we generate prediction intervals as in GAM regressions. 
We generate 10,000 draws of $z(S(\theta))+\epsilon$ where $z$ is normally distributed with standard deviation equal to the infinitesimal jackknife standard error[@Wager2014] at $S(\theta)$ and $\epsilon$ is a resampled residual; we then take the 2.5 and 97.5 percentile of the $z(S(\theta^*))+\epsilon$ set as our prediction interval.











# Results

## Predictivity

```{r ptable}

format_predictivity<-function(x){
  
  
  ifelse(x>-10,scales::number(x,accuracy=.001),"< -10")
}
  

appendix<-
  predictivity %>%  select(-value,-average) %>%
  mutate(predictivity=format_predictivity(predictivity)) %>%
  mutate(method=factor(method,levels = methods_used,
labels = names(methods_used)))  %>%
  mutate(experiment = factor(experiment,levels=experiments_list,
                             labels=experiment_label)) %>%
  arrange(method) %>%
  filter(method!="Average") %>%
  spread(method,predictivity) %>%
  rename(Experiment=experiment,Parameter=parameter) %>%
  knitr::kable(digits=3,format.args = c("scientific"=FALSE))

```


Table \@ref(tab:predictiontable) summarises the predictivity of each algorithm across all identifiable estimation problems (here defined as those where at least one algorithm achieves predictivity of 0.05 or above).
Estimation by GAM achieves the highest average predictivity and the lowest regret (average distance between its predictivity and the highest predictivity in each simulation). Even so, GAM has the best predictivity only for 30 out of a total of 93 identifiable parameters.

Non-linear methods like Random Forest and Neural Network ABC have high average predictivity and together they account for the top performance of another 35 parameters.
Local-linear ABC is on average a very effective algorithm but its average predictivity is penalized by very large errors in a few experiments (in particular the "Hierarchical Normal Mean").



```{r predictiontable}


# median loss from the best
best<-
  predictivity %>% group_by(experiment,parameter) %>%
  filter(predictivity==max(predictivity)) %>%
  ungroup() %>%
  select(-method,-value,-average) %>%
  rename(best=predictivity)


# also we can look at median improvement over lm alone
lms<-
  predictivity %>% group_by(experiment,parameter) %>%
  filter(method=="lm") %>%
  ungroup() %>%
  select(-method,-value,-average) %>%
  rename(lmp=predictivity)

## big table with everything
point_table<-
  left_join(predictivity,best,by=c("experiment","parameter")) %>%
  left_join(lms) %>%
  select(-value,-average) %>%
  mutate(change_from_best=(predictivity-best)/best) %>%
  mutate(change_from_lm=(predictivity-lmp)/lmp) %>%
  group_by(experiment,parameter) %>%
  mutate(was_best=(predictivity==max(predictivity))) %>%
  ungroup() %>%
  filter(best>.05) %>% #there has to be some identification!
  group_by(method) %>%
  summarise(
    times_best=sum(was_best),
            mean_from_best=format_predictivity(mean(change_from_best)),
            mean_predictivity = format_predictivity(mean(predictivity))) %>%
  mutate(method=factor(method,levels = methods_used,
labels = names(methods_used))) 
point_table %>%
    arrange(method) %>% 
  filter(method!="Average") %>%
  knitr::kable(digits = 3, col.names = c("Algorithm",
                                         "# of times highest predictivity",
                                         "Regret",
                                         "Average predictivity"),
               caption = "Table showing for each algorithm how many parameters were best estimated by that algorithm; regret, defined as the average % loss between the predictivity of the algorithm and the best predictivity in each estimation; the average predictivity overall. Local-linear ABC average is distorted by its poor performance in the 'Hierarchical Normal Mean' simulation. Only estimations for which at least one method achieved predictivity above 0.05 were considered.",format="pandoc")

# 
# ggplot(predictivity %>% filter(predictivity>-1)) +
#   geom_histogram(aes(x=predictivity,fill=method),col="white") +
#   facet_wrap(method~.)


## median predictivity is not very good. all it does is put all its weight on the "median" difficulty problem

```

Table \@ref(tab:failuretable) lists the number of identification failures: parameters for which at least another algorithm achieved predictivity of .1 but the algorithm in the table achieved predictivity below .05.
Most parameters are either identified by all the algorithms or none of them.
Local-linear regression struggled with the "natural mean hierarchy" simulation.
Linear regression failed to estimate the $b$ parameter from the Lotka-Volterra models, the $\sigma$ parameter from the normal distribution and the $A$ parameter from the ecological traits model.
Random forests failed to identify $\mu$ and $\delta$ from the RBC macroeconomic model.


```{r failuretable}

left_join(predictivity,best,by=c("experiment","parameter")) %>%
  filter(method!="average") %>%
  filter(best>.1 & predictivity <.05) %>%
  mutate(method=factor(method,levels = methods_used,
labels = names(methods_used)))%>%
  group_by(method) %>%
  summarise(failures=n()) %>%
  ungroup() %>%
  complete(method,fill=list(failures=0)) %>%
  filter(method!="Average") %>%
  arrange(method) %>%
  knitr::kable(
    col.names = c("Algorithm","Identification Failures"),
    caption = "In this table we tabulate the number of identification failures for each algorithm which we define as estimation problems where at least one other algorithm had predictivity above .1 but this algorithm had predictivity below 0.05",format="pandoc"
  )

```

Figure \ref{fig:crossvictories} compares algorithms pairwise with respect to their predictivity.
Even the "best" algorithm, GAM, has lower predictivity for 30-40% of the parameters against to neural network ABC, local-linear regression ABC and random forests.
Linear regression of degree one wins about half of the comparisons against any ABC.



```{r crossvictories, fig.cap = "Percentage of times algorithm 1 has higher predictivity than algorithm 2 for all estimations where at least one algorithm achieves .1 or more predictivity. A blue cell means that algorithm 1 performs generally better, a red cell means that algorithm 2 does."}

victories<-  left_join(predictivity,best,by=c("experiment","parameter")) %>%
  filter(best>.1)


NUMBER_OF_ROWS<-victories %>% group_by(experiment,parameter) %>% summarise(n()) %>% nrow()


check_victories<-function(method1,method2){
  
#   print(paste(method1,method2,sep="-"))
  methods<-c(method1,method2)
  
  if(method1==method2){
    value = NaN}
  else {
    value =  
      victories %>% filter(method %in% methods) %>% 
    group_by(experiment,parameter) %>% 
    filter(predictivity==max(predictivity)) %>% 
    group_by(method) %>% summarise(victories=n()) %>% 
    filter(method==method1) %>%
    pull(victories) / NUMBER_OF_ROWS
    if(length(value) ==0 ) #if there are no victories
      value<-0
  }
#   print(paste(method1,method2,value,sep="-"))
  
  
  return(data.frame(
    method1 = method1,
    method2=method2,
    victory_rate=value
  ))
}

data<-NULL
for(method1 in victories$method %>% unique()){
  for(method2 in victories$method %>% unique()){
    data<-rbind(data,
                check_victories(method1,method2))
}
}

ggplot(data %>% filter(method1!="average") %>% filter(method2!="average") %>%
         mutate(victory_rate = round(victory_rate,2)) %>%
           mutate(method1=factor(method1,
                                 levels = methods_used,
labels = names(methods_used))) %>%
                  mutate(method2=factor(method2,levels = methods_used,
labels = str_replace_all(names(methods_used),pattern = " ",replacement = "\n"))) 
       
       ) +
  geom_tile(
    aes(x=method2,y=method1,fill=victory_rate)
  ) +
  scale_fill_gradient2(low="red",mid="white",high="blue",midpoint = 0.5,guide=FALSE) +
  geom_label(aes(x=method2,y=method1,label=ifelse(is.nan(victory_rate),NA,
                   scales::percent(victory_rate)))
             
             ) +
  xlab("Algorithm 2") +
  ylab("Algorithm 1")


```

It would be wrong however to infer that the choice of estimation algorithm is unimportant and one is better off spending their computational budget on running more simulations.
To prove this we focus on all parameters that were estimated both with 1,250 and 5,000 training simulations. We run a linear mixed effect model where predictivity is the dependent variable and the parameter, the algorithm and the training data size are random effects. 
Table \@ref(tab:bigvssmall) shows that the coefficient associated with training data size is smaller (in absolute terms) than those associated with most algorithms.
Finding the best algorithm then is usually better than quadrupling the training data set.



```{r bigvssmall}
anoza<-predictivity %>% separate(experiment,into=c("name","size"),sep="_") %>% filter(size %in% c("small","medium","big")) %>% mutate(size=str_replace_all(size,"medium","big")) %>%
  filter(method!="average") %>% 
  filter(predictivity>-1) %>%
#  filter(method %in% c("loess","neuralnet","loclin","rfboot")) %>%
  mutate(task = paste(name,parameter,sep="_"))


coef_breaks<-
  c("(Intercept)",
"methodloclin",
"methodloess",
"methodneuralnet",
"methodrejection",
"methodrf",
"methodrfboot",
"methodsabc_1d",
"methodsabc_4d",
"sizesmall",
"sd_(Intercept).task",
"sd_Observation.Residual"
)

coef_names<- c(
  "Intercept",
"Local-linear ABC",
"GAM",
"Neural Network ABC",
"Rejection ABC",
"Quantile Random Forest",
"Regression Random Forest",
"Semi-automatic ABC 1D",
"Semi-automatic ABC 4D",
"Small Training Size",
"sd_(Intercept).task",
"sd_Observation.Residual"
)
  
  



testo<-lmer(predictivity~(1|task)+method+size,data=anoza) 

broom::tidy(testo) %>%
  filter(!str_detect(term,"sd")) %>%
  mutate(term=factor(term,levels = coef_breaks,labels = coef_names)) %>%
  select(-group) %>%
  knitr::kable(col.names=c("Term","Estimate","Std. Error","t"),digits=3,
               caption = "Coefficients of running a linear mixed effects model regressing predictivity on algorithm used and size of the training sample (both fixed effects) with each estimation as a random intercept. Algorithm effects are with respect to linear regression.",format="pandoc" ) 

# ggplot(anoza %>%
#          #ignore comically small values
#          filter(predictivity > -10))+
#   geom_boxplot(aes(x=task,y=predictivity,fill=size)) + 
#   xlab("Algorithm 1") + 
#   ylab("Algorithm 2")#+
# #  geom_point(aes(x=name,y=predictivity,col=size,group=size),position="dodge2")
# 

# the first value tells you how much is explained by the fixed effects vs the second which is the total explaination---> it is mostly question dependent
#MuMIn::r.squaredGLMM(testo)
```


The appendix contains a table with the predictivity for all parameters generated by each algorithm. 

## Coverage

Results on the quality of confidence intervals are easier to interpret, as shown in table \@ref(tab:coveragetable).
Using bootstrap prediction intervals as confidence intervals is superior to both using ABC and quantiles.
In terms of coverage error, the absolute difference between 95% and the proportion of parameters that out of sample are within the generated intervals, GAM  is the most accurate 42% of the time and have the lowest median error while linear regressions have the lowest mean coverage error.
Regression adjusted ABC and quantile random forests make the largest coverage errors.


```{r coveragetable}
## who's best in terms of confidence intervals
bests<-
  coverage_table %>% group_by(experiment,parameter) %>%
  mutate(coverage_error = abs(value-.95)) %>%
  filter(coverage_error==min(coverage_error)) %>%
  group_by(method) %>%
  summarise(times_best=n())

## what about the median coverage?
median_coverage<-
  coverage_table %>% group_by(method)  %>%
  mutate(coverage_error = abs(value-.95)) %>%
  summarise(median = median(coverage_error),
            mean = mean(coverage_error)
            ) 

left_join(bests,
          median_coverage) %>%
  filter(method!="average") %>%
  mutate(method=factor(method,levels = methods_used,
labels = names(methods_used))) %>%
  ungroup() %>%
  arrange(method) %>%  
knitr::kable(
    col.names=c("Algorithm","# of times most accurate","Median Coverage Error","Mean Coverage Error"),digits=4,
    caption = "Table showing for each algorithm median and average coverage error: the absolute difference between 95% and the proportion of parameters actually falling within the algorithm's stated 95% confidence interval (out of sample). The lower the error the more precise the algorithm. For each algorithm we also list the number of parameters for which the stated coverage was the most accurate out of sample compared to the other algorithms",format="pandoc"
  )
  

# 
# pareto<-left_join(median_coverage,median_predictivity)
# ggplot(
#   pareto,aes(x=-coverage_error,y=predictivity)
# ) +
#   geom_point()  +
#   geom_text(aes(label=method),hjust=0,vjust=0)

```


# Discussion

This paper provides two key results.
First, if we are concerned primarily with the quality of point estimates, there is no substitute for trying multiple algorithms and rank them by cross-validation.
GAM regressions do provide a good starting point.
Second, bootstrap prediction intervals are the best method to generate confidence intervals around parameter estimates.

The key advantage of reference table algorithms is that the same reference table can be used to train and test all algorithms at once.
This is what makes cross-validation feasible since running simulations is usually the most expensive part of parameter estimation [accounting for 80% of the computational time in the simple agent-based model of @Grazzini2015].
Compare this with search based methods such as simulated minimum distance: to cross-validate we would need to search the parameter space from scratch for each row of the testing set. 
This is unfeasible for all but the simplest models.

We know of no agent-based model that used cross-validation to choose how to estimate its parameters [with the exception of the comparison between ABC MCMC and simulated minimum distance in @Grazzini2017].
The common approach seems to be to pick one estimation method and apply it.
We have proven here that this is suboptimal: no estimation method seems to be a priori better than the others.

Papers proposing a new estimation algorithms tend to showcase their approach against one or two examples.
It would help the literature to have a larger, standardized set of experiments to gauge any newcomer.
We hope this paper and its code repository to be a first step.
However it may be impossible to find an estimation algorithm that is always best and we should prioritize methods for which cross-validation can be done without having to run more simulations.

The no free lunch theorem[@Wolpert1995] argues that when averaging over the universe of all search problems all optimization algorithms (including random search) perform equally.
A supervised learning version of the same [@Wolpert2011] suggests that "on average" all learning algorithms and heuristics are equivalent.
These are deeply theoretical results whose practical applications are limited: nobody has ever suggested abandoning cross-validation because of it, for example.
However, some weak form of it seems to hold empirically when parametrizing: for each estimation algorithm there is a simulation parameter for which it does best.




# Appendix {-}

## Full Predictivity Table {-}

```{r}
appendix
```



# Acknowledgments {-}

This research is funded in part by the Oxford Martin School, the David and Lucile Packard Foundation, the Gordon and Betty Moore Foundation, the Walton Family Foundation, and Ocean Conservancy.  

## Bibliography {-}